# Health Economics: Economic Evaluation III â€“ Appraising Economic Evaluations Using CHEERS 2022  
**MQB7014: Health Economics**  
**Master of Public Health (MPH) Programme, University of Malaya**  
**Prepared by: Dr Ainol Haniza Kherul Anuwar**  
**DDS (UGM), MCOH (Distinction) (Malaya), DrDPH (Malaya)**  
**Department of Community Oral Health & Clinical Prevention, Faculty of Dentistry, Universiti Malaya**  
**Credit: Prof Dr Maznah Dahlui, Department of Social & Preventive Medicine, Faculty of Medicine, Universiti Malaya**  
**Lecture Date: June 12, 2025**

---

The content focuses on the **Consolidated Health Economic Evaluation Reporting Standards (CHEERS) 2022 checklist** as a tool for appraising the quality of **economic evaluation reporting**, ensuring **transparency**, **clarity**, and **policy relevance**.

---

## ğŸ“š 1. Introduction to CHEERS 2022

**Economic Evaluation III** focuses on critically appraising **economic evaluations** using the **Consolidated Health Economic Evaluation Reporting Standards (CHEERS) 2022 checklist**. This lecture equips students with tools to assess the **quality**, **transparency**, and **reproducibility** of published economic evaluation studies, ensuring they are **policy-relevant** for health systems, including Malaysiaâ€™s. The **CHEERS 2022 checklist** is a standardized framework widely used by **Health Technology Assessment (HTA)** agencies (e.g., NICE, MaHTAS), **peer-reviewed journals**, and **researchers** to enhance reporting quality.

### ğŸ¯ Learning Outcomes
By the end of this session, students should be able to:
- **Understand** the purpose and structure of the **CHEERS 2022 checklist**.
- **Identify** the key domains of **high-quality economic evaluation reporting**.
- **Apply** CHEERS to appraise published **economic evaluation studies**.
- **Recognize** common **reporting gaps** and their implications.

---

## ğŸ©º 2. Recap of Economic Evaluation II

### ğŸ“– Key Concepts from EE II
- **Costing**:
  - **Types**:
    - **Direct Medical**: Costs incurred by the health system/provider (e.g., chemotherapy drugs, oncologist fees, infusion room, lab tests) ğŸ©º.
    - **Direct Non-Medical**: Non-medical expenses due to seeking care (e.g., patient travel, parking, accommodation near hospital) ğŸš—.
    - **Indirect**: Productivity losses due to illness or treatment (e.g., sick leave during chemo cycles, early retirement due to fatigue) ğŸ’¼.
    - **Intangible**: Non-financial costs hard to quantify (e.g., anxiety, pain, hair loss stigma, emotional distress) ğŸ˜Ÿ.
  - **Costing Methods**:
    - **Micro-Costing**: Itemizes and values each resource in detail, used for high precision (e.g., alongside clinical trials).
    - **Gross-Costing**: Uses average cost per service unit, suitable for budget planning or secondary data.
  - **Perspective**: Determines which **costs** are included (e.g., societal includes all costs, health system focuses on direct medical costs).
- **Outcome Measurement**:
  - **Cost-Effectiveness Analysis (CEA)**: Uses **natural units** (e.g., life-years gained, cases prevented).
  - **Cost-Utility Analysis (CUA)**: Uses **utility-based measures** like **Quality-Adjusted Life Years (QALYs)** or **Disability-Adjusted Life Years (DALYs)** âš–ï¸.
  - **Cost-Benefit Analysis (CBA)**: Outcomes monetized (e.g., RM value of productivity).
  - **Utility Values**: Essential for QALY calculations, often derived from tools like **EQ-5D**.

### ğŸ“œ Key Takeaway
- *â€œEven the most precise analysis is only as useful as its clarity, economic evaluation must be transparent to be usable.â€* (Slide 6, repeated due to OCR error).
  - **Transparency** ensures **policymakers**, **HTA agencies**, and **researchers** can trust and apply evaluation results.

### ğŸ“¢ Note on Slide 6
- **Issue**: Slide 6 repeatedly states *â€œKEY TAKEAWAY: Even the most precise analysis is only as useful as its clarity, economic evaluation mustâ€*, truncated multiple times (14,271 characters). This is likely an OCR error, as the repetition is non-informative. The takeaway is interpreted as emphasizing **transparency** in reporting, consistent with the lectureâ€™s focus.

---

## ğŸ“‹ 3. Why Critical Appraisal Matters

### ğŸ“– The Problem
Many **economic evaluations** appear robust, using **statistically rigorous models** and **impressive Incremental Cost-Effectiveness Ratios (ICERs)**, but **incomplete reporting** undermines their **credibility**. Without clear reporting, decision-makers cannot assess the **validity** or **relevance** of findings.

- **Issues**:
  - Evaluations may look **rigorous** and use **advanced modeling** but fail to report **key details** (Slide 7).
  - **Example**: A study stating â€œQALYs were estimatedâ€ without specifying:
    - **Which utility tool?** (e.g., EQ-5D, SF-6D).
    - **Whose preferences?** (e.g., population or country-specific value set).
    - **Time horizon or discounting?**
  - Such omissions make even a **correct ICER** uninterpretable (Slide 12).
- **Poorly Reported Evaluations** (Slide 8, incomplete): Lack of clarity hinders their use in **decision-making**.

### ğŸŒŸ Why It Matters for Public Health and Policy
- **Impact**: **Poorly reported evaluations** cannot inform:
  - **Health Technology Assessments (HTA)**.
  - **Budget impact analysis** and **resource allocation**.
  - **Reimbursement decisions** and **coverage recommendations**.
  - **National clinical guidelines** and **service prioritization** (Slide 36).
- **Key Message**: *â€œA strong economic evaluation is not just about having the right model, itâ€™s about reporting it clearly, so others can evaluate and trust your results.â€* (Slide 9).

### ğŸ“œ CHEERS in Policy and Research
- **Widely Used By** (Slide 13):
  - **HTA agencies** (e.g., NICE in the UK, MaHTAS in Malaysia).
  - **Peer-reviewed journals** for manuscript preparation.
  - **Researchers** to align with **evidence transparency** in health system decision-making.
- **Key Takeaway**: *â€œIf itâ€™s not reported, it didnâ€™t happen. CHEERS ensures every essential part of an economic evaluation is visible, understandable, and usable.â€* (Slide 13).

---

## ğŸ“š 4. What is CHEERS 2022?

### ğŸ“– Definition and Purpose
- **CHEERS (Consolidated Health Economic Evaluation Reporting Standards) 2022**: A **reporting checklist** that enhances **clarity**, **transparency**, and **policy usability** of economic evaluations (Slides 10, 14).
- **Purpose**: Ensures every essential part of an economic evaluation is **visible**, **understandable**, and **usable** for stakeholders like **HTA agencies**, **journals**, and **policymakers**.

---

## ğŸ§  5. CHEERS 2022 Structure

### ğŸ“– Overview
The **CHEERS 2022 checklist** comprises **28 items** organized into **6 domains** to guide comprehensive reporting of **economic evaluations**. The **Methods section** is emphasized as the **â€œengine roomâ€** of an evaluation, determining whether findings are **understandable**, **reproducible**, and **policy-relevant** (Slide 16). While the slides provide specific items (e.g., 3â€“12, 18â€“21, 25), the full checklist is supplemented from the published source (Husereau et al., 2022, *Value in Health*) to ensure **no truncation**, as per instructions.

### ğŸ“‹ Complete CHEERS 2022 Checklist
The **6 domains** and **28 items** are:
1. **Title and Abstract**:
   - **1. Title**: Identify the study as an economic evaluation or use terms like â€œcost-effectivenessâ€ or â€œcost-utilityâ€ to aid discovery.
   - **2. Abstract**: Provide a structured summary of objectives, perspective, methods, results, and conclusions.
2. **Introduction**:
   - **3. Background and Objectives**: Give the context for the study, the study question, and its practical relevance for decision-making in policy or practice.
3. **Methods**:
   - **4. Health Economic Analysis Plan**: Indicate whether a health economic analysis plan was developed and where available.
   - **5. Study Population**: Describe characteristics of the study population (e.g., age range, demographics, socioeconomic, or clinical characteristics).
   - **6. Setting and Location**: Provide relevant contextual information that may influence findings.
   - **7. Comparators**: Describe the interventions or strategies being compared and why chosen.
   - **8. Perspective**: State the perspective(s) adopted by the study and why chosen.
   - **9. Time Horizon**: State the time horizon for the study and why appropriate.
   - **10. Discount Rate**: Report the discount rate(s) and reason chosen.
   - **11. Selection of Outcomes**: Describe what outcomes were used as the measure(s) of benefit(s) and harm(s).
   - **12. Measurement of Outcomes**: Describe how outcomes used to capture benefit(s) and harm(s) were measured.
   - **13. Valuation of Outcomes**: Describe methods to value outcomes (e.g., utility weights for QALYs).
   - **14. Measurement and Valuation of Resources and Costs**: Describe approaches to measure resource use and assign unit costs.
   - **15. Currency, Price Date, and Conversion**: Report the currency, price date, and conversion methods.
   - **16. Rationale and Description of Model**: If applicable, describe the model type (e.g., decision tree, Markov) and why chosen.
   - **17. Analytics and Assumptions**: Describe analytical methods and key assumptions.
   - **18. Characterizing Heterogeneity**: Describe methods used for estimating how results vary for sub-groups.
   - **19. Characterizing Distributional Effects**: Describe how impacts are distributed across different individuals or adjustments made for priority populations.
   - **20. Characterizing Uncertainty**: Describe methods to characterize sources of uncertainty in the analysis.
   - **21. Approach to Engagement with Patients and Others**: Describe approaches to engage patients, service recipients, the public, communities, or stakeholders (e.g., clinicians, payers) in the study design.
4. **Results**:
   - **22. Study Parameters**: Report values, ranges, and sources for all model inputs.
   - **23. Base-Case Analysis**: Report results of the base-case analysis (e.g., ICERs, costs, outcomes).
   - **24. Sensitivity and Scenario Analysis**: Report results of sensitivity and scenario analyses.
   - **25. Effect of Engagement with Patients and Others**: Describe the impact of stakeholder engagement on the study.
5. **Discussion**:
   - **26. Study Findings, Limitations, Generalizability, and Current Knowledge**: Summarize findings, limitations, generalizability, and how results fit with current knowledge.
6. **Other**:
   - **27. Source of Funding**: Describe how the study was funded and the role of the funder.
   - **28. Conflicts of Interest**: Report any conflicts of interest for study authors.

### ğŸ“Š Sample CHEERS Items (Methods Section, Slide 17)
- **Focus**: The **Methods section** is critical, as it ensures **understandability**, **reproducibility**, and **policy relevance**.
- **Examples**: 

| **CHEERS Item** | **Strong Reporting Example** | **Weak/Incomplete Example** |
|-----------------|-----------------------------|----------------------------|
| **Perspective** | â€œA health system perspective was adopted, including only direct medical costs as per MaHTAS guidelines.â€ | â€œCosts were included.â€ |
| **Discount Rate** | â€œBoth costs and QALYs were discounted at 3% annually, consistent with WHO guidelines.â€ | â€œDiscounting was applied.â€ |
| **Health Outcomes** | â€œQALYs were estimated using EQ-5D-5L with Malaysian value set.â€ | â€œQALYs were used.â€ |
| **Resource Use & Costs** | â€œResource use was obtained from clinical records, and unit costs were sourced from MOH tariffs.â€ | â€œCosts were estimated.â€ |
| **Analytical Methods** | â€œA Markov model with 1-year cycles simulated transitions between health states.â€ | â€œA model was used to project results.â€ |
| **Sensitivity Analysis** | â€œOne-way and probabilistic sensitivity analyses were conducted on drug cost, utilities, and transition probabilities.â€ | â€œSensitivity analysis was done.â€ |

- **Question for Appraisal**: *Could a policymaker or researcher reproduce this study based on whatâ€™s reported in the methods section?* (Slide 18).

### ğŸ“¢ Notes on Incomplete Slides
- **Slide 15**: Contains repetitive â€œ1â€s (1,545 characters), likely an OCR error. It is assumed to reference the **CHEERS 2022 structure** (6 domains, 28 items), but no specific content is provided. The full checklist is included above from the published source to avoid truncation.
- **Slides 20â€“22, 27, 30, 37**: Contain minimal content (#, empty, or â€œ|â€), likely placeholders or OCR errors. No assumptions are made beyond their implied role as placeholders for additional checklist items or examples.
- **Slide 5**: Empty except for â€œWHAT WE COVERED IN EE II,â€ likely a continuation of Slide 4â€™s recap, with no additional content.
- **Slide 3**: Empty except for date (June 12, 2025), likely a title or transition slide.

---

## ğŸ” 6. Applying the CHEERS Checklist

### ğŸ“– How to Use CHEERS
- **Process** (Slide 19):
  1. **Read the Article**: Review section by section (**Title/Abstract**, **Introduction**, **Methods**, **Results**, **Discussion**).
  2. **Match to CHEERS Items**: Align each section with the relevant **28 checklist items** across **6 domains**.
  3. **Evaluate Reporting**:
     - Was the item **clearly and fully reported**?
     - Was it **mentioned but unclear/incomplete**?
     - Was it **missing altogether**?
- **Goal**: Assess **clarity**, **transparency**, and **reproducibility** to ensure the study is **usable** for decision-making.

### ğŸŒŸ Importance
- **Enhances Trust**: Transparent reporting builds **confidence** in results for **policymakers** (e.g., MaHTAS) and **HTA agencies**.
- **Policy Impact**: Clear studies are more likely to inform **resource allocation**, **reimbursement**, and **guidelines**.

---

## ğŸ“Š 7. Case Study: Lung Cancer Screening in Germany

### ğŸ“– Study Overview
- **Title**: *Cost-utility analysis of a potential lung cancer screening program for a high-risk population in Germany: A modelling approach* (Slide 23).
- **Authors**: Florian Hofer, Hans-Ulrich Kauczor, Tom Stargardt.
- **Source**: Not fully provided (truncated text), but cited as a published study.

### ğŸ“‹ Article Information (Slide 24)
- **Keywords**: Early detection, LDCT screening, health economic evaluation, cost-effectiveness analysis, Markov modeling, Bayesian calibration.
- **Background**: Lung cancer is the **leading cause of cancer death** in Germany. While randomized trials have evaluated **effectiveness** of screening, **cost-effectiveness** evidence is scarce.
- **Objective**: Evaluate the **cost-effectiveness** of a population-based **lung cancer screening program** from a **public payer perspective** for a **high-risk population** (heavy smokers, â‰¥20 cigarettes/day, aged 55â€“75).
- **Methods**:
  - **Model**: Two **Markov models** comparing:
    - **Annual screening program** vs. **standard clinical care**.
  - **Population**: Heavy former/current smokers (â‰¥20 cigarettes/day, aged 55â€“75).
  - **Treatment Paths**: Five paths based on **stage at diagnosis**, per German clinical guidelines for lung cancer diagnosis and treatment.
  - **Outcomes**: **Costs**, **life years saved**, **QALYs**.
  - **Data Sources**: Values for input parameters taken from the literature.
  - **Time Horizon**: 60 cycles (3-month cycle length).
  - **Analyses**: **Deterministic** and **probabilistic sensitivity analyses**.
- **Results**:
  - **Base Case**: Annual screening increased **incremental costs** (â‚¬1,153 per person) compared to standard clinical care.
  - Gained **0.06 life years** and **0.04 QALYs** per person.
  - **ICER**: Truncated in slides (â€œâ‚¬1,3, 3, 3, â€¦â€), implying calculation as cost per QALY, but exact value unavailable due to OCR error.

---

## ğŸš« 8. Common Reporting Gaps in Economic Evaluations

### ğŸ“– Overview
Incomplete or inconsistent reporting undermines the **credibility** and **usability** of economic evaluations. The lecture identifies the **top 5 reporting gaps**, critical for ensuring **policy-relevant** studies (Slides 31â€“35).

1. **Perspective Not Stated** (Slide 31):
   - **Issue**: Many studies fail to clarify whether the analysis uses a **societal**, **payer**, or **provider** perspective.
   - **Why It Matters**: Perspective determines **which costs** and **outcomes** should be included, affecting **interpretation**.
   - **Example**: A study includes only **drug costs** but claims **societal relevance**, misleading stakeholders about broader impacts.

2. **No Justification for Time Horizon or Discount Rate** (Slide 32):
   - **Issue**: 
     - **Time horizons** are sometimes chosen **randomly**.
     - **Discounting** may be applied without explanation or omitted entirely.
   - **Why It Matters**:
     - Short timeframes can **misrepresent** costs/benefits of chronic diseases.
     - Discounting ensures **future costs/outcomes** are weighted appropriately to reflect time preference.
   - **Example**: Modeling **lifetime outcomes** for breast cancer without reporting **discounting**, leading to inflated QALY estimates.

3. **Omission of Sensitivity Analysis** (Slide 33):
   - **Issue**: Studies may lack **sensitivity analysis** or test only **trivial variables**, despite the term being misspelled as â€œSensitytyy Anayyâ€ (OCR error).
   - **Why It Matters**: Sensitivity analysis tests **robustness** under uncertainty, critical for **policy confidence** in results.
   - **Example**: Not testing variations in **utility scores** for high-impact side effects, reducing trust in cost-effectiveness conclusions.

4. **Incomplete Model Structure Explanation** (Slide 34):
   - **Issue**: Models (e.g., Markov) are used without detailing **structure**, **assumptions**, or **parameters** (e.g., transition probabilities). Slide 34â€™s repetitive â€œIncomplete Model Structure Explanationâ€ (15,324 characters) is an OCR error but indicates this gap.
   - **Why It Matters**: Without clarity, models are **opaque** and **non-reproducible**, undermining their use in policy.
   - **Example**: Stating â€œa model was usedâ€ without specifying **health states**, **cycle length**, or **transition probabilities**.

5. **Utility Sources Not Reported** (Slide 35):
   - **Issue**: Studies state **QALYs** were calculated but omit details on:
     - **Instruments** used (e.g., EQ-5D, SF-6D).
     - **Population** or **country-specific value sets**.
   - **Why It Matters**: **Utility weights** vary by setting and significantly impact **QALY estimates**, affecting cost-effectiveness results.
   - **Example**: Stating â€œutility weights from the literatureâ€ without citation, preventing verification.

---

## ğŸŒŸ 9. Why Reporting Quality Equals Policy Impact

### ğŸ“– Importance
- **Economic evaluations** inform critical decisions in (Slide 36):
  - **Health Technology Assessments (HTA)**.
  - **Budget impact analysis** and **resource allocation**.
  - **Reimbursement decisions** and **coverage recommendations**.
  - **National clinical guidelines** and **service prioritization**.
- **Impact of Poor Reporting**: Unclear studies cannot be used by **policymakers**, regardless of result validity, reducing their **policy impact**.

### ğŸ“‹ Key Takeaways (Slide 38)
- **CHEERS 2022** enhances **clarity**, **transparency**, and **policy usability** of economic evaluations.
- **Appraising with CHEERS**:
  - Helps assess **what was done** and **how clearly it was communicated**.
  - Is distinct from **technical peer review** but equally essential for ensuring usability.
- **Common Gaps**:
  - Missing **perspective**, **discounting**, and **sensitivity analyses**.
  - Poor explanation of **models** and **QALY derivation**.
- **Transparent Reporting**: Builds **trust** and increases **policy uptake**, ensuring evaluations influence **real-world decisions**.

---
